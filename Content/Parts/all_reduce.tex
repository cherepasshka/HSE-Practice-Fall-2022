%!TEX root = ../main

\newpage
\section{Алгоритм <<All-reduce>>}

Чем больше датасет - тем больше времени тратится на обучение модели по нему. Распределенный синхронный стохастический градиентный спуск - это одно из возможных решений этой проблемы.\cite{DBLP}

\subsection{Описание}
Предположим, у нас есть 8 GPU и мы хотим обучать модель, используя SGD на мини-батчах размера n.

В первую очередь, мы увеличиваем размер батчей до kn, чтобы затем распределить его на k воркеров.
И здесь мы задаемся первым вопросом - а не ухудшится ли точность модели от увеличения размера батча?

Обычно функция потерь для модели имеет следующий вид:
\[L(w) = \frac{1}{|X|} \sum_{x \in X}^{} l(x, w)\]

Здесь $w$ - это параметры модели, $X$ - обучающая выборка, а $l$ -- некоторая функция ошибки,
которая в том числе может включать в себя нормализацию.

Тогда один шаг обучения SGD имеет следующий вид:
\[w_{t + 1} = w_t - \eta \frac1n \sum_{x \in \mathcal{B}}^{} \nabla l(x, w_t)\]

Здесь $\eta$ -- это скорость обучения, гиперпараметр модели, а $\mathcal{B}$ -- текущий батч.

Заметим, что если мы увеличим размер батча в $k$ раз, то количество слагаемых в сумме уменьшится в
$k$ раз. Отсюда вытекает идея: давайте увеличим скорость обучения в $k$ раз, чтобы скомпенсировать
это. И на реальных данных оказывается, что это действительно неплохой подход, который сохраняет
точность модели, за исключением одного случая - начала обучения.

В самом начале, когда модель меняется больше всего, предлагается начать со старой скорости обучения,
равной $\eta$, а затем каждую эпоху увеличивать ее на константу так, чтобы к пятой эпохе она стала
равной $k \eta$. А далее сохраним эту скорость обучения.

Теперь, когда мы научились увеличивать размер батча без особых потерь, будем распределять каждый
батч между воркерами, а затем синхронизировать полученные результаты между ними.

\subsection{Анализ}

Обучалась модель ResNet-50 на датасете ImageNet в течение 90 эпох. В результате модель обучилась за
1 час с размером батча 8192.

Для сравнения, на одной видеокарте с размером батча 256 модель обучается около 29 часов.

При этом самая большая ошибка обучения в датасете увеличилась не более, чем на 0.3\%.

